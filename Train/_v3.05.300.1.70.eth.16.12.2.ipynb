{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "os.environ['TF_ENABLE_ONEDNN_OPTS'] = str(0)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras  # tf.keras\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import ipyparams\n",
    "import json\n",
    "\n",
    "from test_utility import *\n",
    "\n",
    "print(\"python\", sys.version)\n",
    "for module in mpl, np, pd, sklearn, tf, keras:\n",
    "    print(module.__name__, module.__version__)\n",
    "    \n",
    "assert sys.version_info >= (3, 5) # Python ≥3.5 required\n",
    "assert tf.__version__ >= \"2.0\"    # TensorFlow ≥2.0 required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "notebookName = None\n",
    "try:\n",
    "    notebookName = os.path.basename(globals()['__vsc_ipynb_file__'])\n",
    "except:\n",
    "    pass\n",
    "\n",
    "if type(notebookName) == str and len(notebookName.split('.')) > 1:  # looks like running on VSCode\n",
    "    pass\n",
    "else:  # looks like running on a browser\n",
    "    notebookName = ipyparams.notebook_name\n",
    "assert type(notebookName) == str and len(notebookName.split('.')) > 2\n",
    "notebookName = \".\".join(notebookName.split('.')[:-2])   # -2: gets rid of train version and file extention.\n",
    "print(notebookName)\n",
    "\n",
    "data_model = notebookName  #\"vm03.05.250.11.80.100.16.14.1\"\n",
    "assert len(notebookName.split('.')) >= 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_data = \"/mnt/data/Trading/\"\n",
    "\n",
    "#===================================================================== Dataset\n",
    "\n",
    "Nx = 300 # ------------- test\n",
    "Ny = 1\n",
    "Ns = 2 #--------------------- test\n",
    "BatchSize = 128 * 10\n",
    "\n",
    "Shift = 0 # < 2 history: 0,  1, 0, 1\n",
    "\n",
    "CandleFile = \"18-01-01-00-00-23-05-20-20-23-5m\"\n",
    "CandleFile2 = \"23-05-19-00-05-23-06-22-02-40-5m\"\n",
    "SmallSigma = 1\n",
    "LargeSigma = 30\n",
    "eFreeNoLog = True\n",
    "\n",
    "shuffle_batch = 30  # Keep it small to speed up model loading.\n",
    "\n",
    "dir_candles = os.path.join(dir_data, \"Candles\")\n",
    "\n",
    "Volume_Volatility = 1\n",
    "All_Field_Names = ['ClosePrice', 'BaseVolume', 'BuyerBaseVolume']\n",
    "min_true_candle_percent_x = 70\n",
    "chosen_markets_x = []\n",
    "chosen_fields_names_x = ['ClosePrice'] #, 'BaseVolume']\n",
    "min_true_candle_percent_y = 70\n",
    "assert min_true_candle_percent_x == min_true_candle_percent_y\n",
    "chosen_markets_y = []\n",
    "chosen_fields_names_y = ['ClosePrice']\n",
    "\n",
    "target_market_names = None\n",
    "# target_market_names = ['NEOUSDT', 'LTCUSDT', 'BTCUSDT', 'ETHUSDT', 'BNBUSDT', 'QTUMUSDT', 'ADAUSDT', 'XRPUSDT']\n",
    "target_market_names = ['ETHUSDT']\n",
    "tarket_market_top_percent = 15\n",
    "\n",
    "Standardization = True\n",
    "Kill_Irregulars = True  # ----------------- pls implement it\n",
    "Time_into_X = True\n",
    "Time_into_Y = False #\n",
    "eFreeNoPlot = True\n",
    "\n",
    "#======================================================================== Model\n",
    "\n",
    "Num_Layers = 16 # Wow\n",
    "Num_Heads = 1   # As we have a single GPU, and we want to a exhaustic attention.\n",
    "Factor_FF = 4\n",
    "repComplexity = 12  # Wower\n",
    "Dropout_Rate = 0.  # You cannot change this once the model is built.\n",
    "\n",
    "dir_Checkpoint = os.path.join(dir_data, \"Checkpoints\")\n",
    "checkpoint_filepath = os.path.join(dir_Checkpoint, data_model)\n",
    "dir_CSVLogs = os.path.join(dir_data, \"CSVLogs\")\n",
    "csvLogger_filepath = os.path.join(dir_CSVLogs, data_model)\n",
    "\n",
    "#======================================================================== Train\n",
    "\n",
    "Epochs_Initial = 5000\n",
    "HuberThreshold = 4.0\n",
    "Checkpoint_Monitor = \"val_loss\"\n",
    "EarlyStopping_Min_Monitor = \"val_loss\"\n",
    "EarlyStopping_Patience = 5000\n",
    "\n",
    "Optimizer = \"adam\"\n",
    "LR_initial = 3.e-6 # default: 1e-4\n",
    "LR_rate = 0./100\n",
    "LR_skip = 0\n",
    "\n",
    "#=============================================================== Checksum\n",
    "\n",
    "params = data_model.split('.')\n",
    "assert int(params[1]) == int(CandleFile.split('-')[-1][:-1])\n",
    "assert int(params[2]) == Nx\n",
    "assert int(params[3]) == Ny\n",
    "assert int(params[4]) == min_true_candle_percent_x\n",
    "assert int(params[6]) == Num_Layers\n",
    "assert int(params[7]) == repComplexity\n",
    "targets = params[5]\n",
    "if targets.isnumeric():\n",
    "    assert target_market_names is None\n",
    "    assert int(targets) == tarket_market_top_percent\n",
    "else:\n",
    "    for target in targets.split(','):\n",
    "        assert (target+'usdt').upper() in target_market_names\n",
    "\n",
    "#============================================================== Consistency\n",
    "folders = [dir_data, dir_candles, dir_Checkpoint, dir_CSVLogs]\n",
    "for folder in folders:\n",
    "    if not os.path.isdir(folder):\n",
    "        os.mkdir(folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    print(gpus)\n",
    "    try:\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "            # tf.config.experimental.set_virtual_device_configuration(\n",
    "            #     gpu,[tf.config.experimental.VirtualDeviceConfiguration(memory_limit=5120)]) # why 5120?\n",
    "            # logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "            # print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)\n",
    "\n",
    "mirrored_strategy = None\n",
    "if len(gpus) > 1: \n",
    "    mirrored_strategy = tf.distribute.MirroredStrategy()\n",
    "    LR_initial = LR_initial * len(gpus) * 3 / 4\n",
    "\n",
    "# tf.config.experimental.set_virtual_device_configuration(\n",
    "#     gpus[0],[tf.config.experimental.VirtualDeviceConfiguration(memory_limit=5120)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==================== Load candle and reports. Data structure depends on TradingBot ====================\n",
    "Candles = np.load( os.path.join( dir_candles, \"table-\" + CandleFile + \".npy\") )\n",
    "with open( os.path.join( dir_candles, \"reports-\" + CandleFile + \".json\"), \"r\") as f:\n",
    "    reports = json.loads(f.read())\n",
    "\n",
    "#==================== Load candle and reports. Data structure depends on TradingBot ====================\n",
    "Candles2 = np.load( os.path.join( dir_candles, \"table-\" + CandleFile2 + \".npy\") )\n",
    "with open( os.path.join( dir_candles, \"reports-\" + CandleFile2 + \".json\"), \"r\") as f:\n",
    "    reports2 = json.loads(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "market = 5\n",
    "Show_Price_Volume_10(Candles[market, :, :], 1, 1, 500)\n",
    "Event_Free_Learning_Scheme_10(Candles[market, :, :], 3, 30, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==================== Format candles data ====================================\n",
    "Candles, CandleMarks, all_market_names, x_indices, y_indices, \\\n",
    "chosen_market_names_x, chosen_field_names_x, chosen_market_names_y, chosen_field_names_y, \\\n",
    "chosen_market_names, chosen_field_names, \\\n",
    "target_markets_names, target_markets = \\\n",
    "get_formed_data_3(\n",
    "        Candles, reports, All_Field_Names, \n",
    "        min_true_candle_percent_x, chosen_fields_names_x, min_true_candle_percent_y, chosen_fields_names_y,\n",
    "        target_market_names, tarket_market_top_percent\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Candles.shape)\n",
    "print(CandleMarks.shape)\n",
    "print(len(all_market_names))\n",
    "print(x_indices)\n",
    "print(y_indices)\n",
    "print(chosen_market_names_x)\n",
    "print(chosen_field_names_x)\n",
    "print(chosen_market_names_y)\n",
    "print(chosen_field_names_y)\n",
    "print(chosen_market_names)\n",
    "print(chosen_field_names)\n",
    "print(target_markets_names)\n",
    "print(target_markets)\n",
    "print(len(chosen_market_names_x), len(chosen_market_names_y), len(target_markets_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "market = all_market_names.index(\"ETHUSDT\")\n",
    "plt.plot(Candles[:, market, 0], label='eth', linewidth=0.5)\n",
    "market = all_market_names.index(\"BNBUSDT\")\n",
    "plt.plot(Candles[:, market, 0], label='bnb', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Candles2, CandleMarks2, missing_names2 = get_conformed_data_3(\n",
    "    Candles2, reports2, chosen_market_names,\n",
    "    All_Field_Names, chosen_field_names\n",
    " )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "market = all_market_names.index(\"ETHUSDT\")\n",
    "joined = np.concatenate([Candles[:, market, 0], Candles2[:, market, 0]], axis=0)\n",
    "plt.plot(joined, label='eth', linewidth=1)\n",
    "market = all_market_names.index(\"BNBUSDT\")\n",
    "joined = np.concatenate([Candles[:, market, 0], Candles2[:, market, 0]], axis=0)\n",
    "plt.plot(joined, label='bnb', linewidth=1)\n",
    "if len(missing_names2) > 0:\n",
    "    market = all_market_names.index(missing_names2[0])\n",
    "    joined = np.concatenate([Candles[:, market, 0], Candles2[:, market, 0]], axis=0)\n",
    "    plt.plot(joined, label=\"1st missing: \" + missing_names2[0][:-len('USDT')].lower(), linewidth=1)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=========================== Get time features ======================\n",
    "start_ts, interval_s, timestamps_abs, Times = get_time_features_2(CandleFile, Candles.shape[0], Candles.dtype)\n",
    "size_time = Times.shape[1]\n",
    "\n",
    "start_ts2, _, timestamps_abs2, Times2 = get_time_features_2(CandleFile2, Candles2.shape[0], Candles2.dtype)\n",
    "assert timestamps_abs2[0] - timestamps_abs[-1] == interval_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Candles.shape, Times.shape)\n",
    "print(timestamps_abs[:5])\n",
    "print(Candles2.shape, Times2.shape)\n",
    "print(timestamps_abs2[:5])\n",
    "# print(Times[:5])    # Agnostic of restart of this training notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#========================= Get sample anchors ==============================\n",
    "sample_anchors_t, sample_anchors_v = get_sample_anchors_3(Candles, Nx, Ny, Ns, seed=523)\n",
    "print(sample_anchors_t.shape, sample_anchors_v.shape)\n",
    "print(sample_anchors_t[:5])\n",
    "assert sample_anchors_t[0] == 206568  \n",
    "assert sample_anchors_t[1] == 407538 \n",
    "\n",
    "sample_anchors_t2, sample_anchors_v2 = get_sample_anchors_3(Candles2, Nx, Ny, Ns, seed=523)\n",
    "print(sample_anchors_t2.shape, sample_anchors_v2.shape)\n",
    "print(sample_anchors_t2[:5])\n",
    "assert sample_anchors_t2[0] == 8802  \n",
    "assert sample_anchors_t2[1] == 974 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "JointCandles, JointMarks, sample_anchors_t, sample_anchors_v = \\\n",
    "    get_joint_data(\n",
    "        Candles, CandleMarks, sample_anchors_t, sample_anchors_v,\n",
    "        Candles2, CandleMarks2, sample_anchors_t2, sample_anchors_v2\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(JointCandles.shape)\n",
    "print(sample_anchors_t.shape, sample_anchors_v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_ts, interval_s, timestamps_abs, JointTimes = get_time_features_2(CandleFile, JointCandles.shape[0], JointCandles.dtype)\n",
    "size_time = Times.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#======================== Get event-free data ========================\n",
    "\n",
    "org_length = JointCandles.shape[0]\n",
    "JointCandles, JointMarks, JointTimes = get_eventfree_data(\n",
    "    JointCandles, JointMarks, JointTimes,\n",
    "    SmallSigma, LargeSigma, all_market_names, chosen_market_names, \n",
    "    All_Field_Names, chosen_field_names,\n",
    "    eFreeNoLog, eFreeNoPlot, Volume_Volatility\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert JointCandles.shape[0] == JointMarks.shape[0]\n",
    "assert JointCandles.shape[0] == JointTimes.shape[0]\n",
    "print(JointCandles.shape, CandleMarks.shape, JointTimes.shape, org_length-JointCandles.shape[0])\n",
    "# print(Candles[-3:]) # Agnostic of restart of this training notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#========================= Standardize data ============================\n",
    "Standard = None\n",
    "if Standardization:\n",
    "    JointCandles, Standard = standardize_2(JointCandles)\n",
    "# print(Candles[-3:]) # Agnostic of restart of this training notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(16,3))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.set_title(\"Features are custom-standardized\" if Standardization else \"Features are not standardized\")\n",
    "for market in range(JointCandles.shape[1]):\n",
    "    for field in range(JointCandles.shape[2]):\n",
    "        ax.plot(JointCandles[:, market, field], label = \"{} @ {}\".format(All_Field_Names[field], all_market_names[market][:-len('USDT')]), lw=0.1)\n",
    "ax.legend(loc = 'upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train, ds_valid, dx, dy = \\\n",
    "get_datasets_3(\n",
    "    JointCandles, Time_into_X, Time_into_Y, JointTimes, \n",
    "    sample_anchors_t, sample_anchors_v,\n",
    "    Nx, x_indices, Ny, y_indices, size_time, target_markets, Shift,\n",
    "    BatchSize, shuffle_batch, shuffle=(len(gpus)<=1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = None\n",
    "\n",
    "if mirrored_strategy is None:\n",
    "    model = build_model_3(\n",
    "        dx, dy, Num_Layers, Num_Heads, Factor_FF, repComplexity, Dropout_Rate,\n",
    "        HuberThreshold, Optimizer, initial=LR_initial, rate=LR_rate, skip=LR_skip\n",
    "    )\n",
    "else:\n",
    "    with mirrored_strategy.scope():\n",
    "        model = build_model_3(\n",
    "            dx, dy, Num_Layers, Num_Heads, Factor_FF, repComplexity, Dropout_Rate,\n",
    "            HuberThreshold, Optimizer, initial=LR_initial, rate=LR_rate, skip=LR_skip\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = get_callbacks(\n",
    "    checkpoint_filepath, Checkpoint_Monitor, \n",
    "    csvLogger_filepath, \n",
    "    EarlyStopping_Min_Monitor, EarlyStopping_Patience\n",
    ")\n",
    "try:\n",
    "    model.load_weights(checkpoint_filepath)\n",
    "    print(\"Loading a checkpoint...done\")\n",
    "except:\n",
    "    print(\"No chekkpoint to load. Fitting initailly...\")\n",
    "    model.fit(\n",
    "        ds_train, # x and y_true\n",
    "        validation_data=ds_valid,\n",
    "        epochs=1, #Epochs_Initial,\n",
    "        callbacks=callbacks\n",
    "    )\n",
    "try:\n",
    "    columns = ('loss', 'val_loss', 'mTA', 'val_mTA')\n",
    "    plot_csv_train_history(csvLogger_filepath, columns, title=data_model)\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model.fit(\n",
    "    ds_train, # x and y_true\n",
    "    validation_data=ds_valid,\n",
    "    epochs=Epochs_Initial,\n",
    "    callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ('loss', 'val_loss', 'mTA', 'val_mTA')\n",
    "plot_csv_train_history(csvLogger_filepath, columns, title=data_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
