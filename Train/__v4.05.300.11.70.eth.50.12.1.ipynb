{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "os.environ['TF_ENABLE_ONEDNN_OPTS'] = str(0)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras  # tf.keras\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import ipyparams\n",
    "import json\n",
    "\n",
    "from test_utility import *\n",
    "\n",
    "print(\"python\", sys.version)\n",
    "for module in mpl, np, pd, sklearn, tf, keras:\n",
    "    print(module.__name__, module.__version__)\n",
    "    \n",
    "assert sys.version_info >= (3, 5) # Python ≥3.5 required\n",
    "assert tf.__version__ >= \"2.0\"    # TensorFlow ≥2.0 required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "notebookName = None\n",
    "try:\n",
    "    notebookName = os.path.basename(globals()['__vsc_ipynb_file__'])\n",
    "except:\n",
    "    pass\n",
    "\n",
    "if type(notebookName) == str and len(notebookName.split('.')) > 1:  # looks like running on VSCode\n",
    "    pass\n",
    "else:  # looks like running on a browser\n",
    "    notebookName = ipyparams.notebook_name\n",
    "assert type(notebookName) == str and len(notebookName.split('.')) > 2\n",
    "notebookName = \".\".join(notebookName.split('.')[:-2])   # -2: gets rid of train version and file extention.\n",
    "print(notebookName)\n",
    "\n",
    "data_model = notebookName  #\"vm03.05.250.11.80.100.16.14.1\"\n",
    "assert len(notebookName.split('.')) >= 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_data = \"/mnt/data/Trading/\"\n",
    "\n",
    "#===================================================================== Dataset\n",
    "\n",
    "Nx = 300 # ------------- test\n",
    "Ny = 11\n",
    "Ns = 5 #--------------------- test\n",
    "BatchSize = 128 * 2 # 9\n",
    "\n",
    "Shift = 2 # 0, 1\n",
    "\n",
    "CandleFile = \"18-01-01-00-00-23-05-20-20-23-5m\"\n",
    "CandleFile2 = \"23-05-19-00-05-23-06-22-02-40-5m\"\n",
    "CandleFile3 = \"23-06-21-00-05-23-06-26-17-36-5m\"\n",
    "SmallSigma = 1\n",
    "LargeSigma = 30\n",
    "eFreeNoLog = True\n",
    "\n",
    "shuffle_batch = 30  # Keep it small to speed up model loading.\n",
    "\n",
    "dir_candles = os.path.join(dir_data, \"Candles\")\n",
    "\n",
    "Volume_Volatility = 1\n",
    "All_Field_Names = ['ClosePrice', 'BaseVolume', 'BuyerBaseVolume']\n",
    "min_true_candle_percent_x = 70\n",
    "chosen_markets_x = []\n",
    "chosen_fields_names_x = ['ClosePrice'] #, 'BaseVolume']\n",
    "min_true_candle_percent_y = 70\n",
    "assert min_true_candle_percent_x == min_true_candle_percent_y\n",
    "chosen_markets_y = []\n",
    "chosen_fields_names_y = ['ClosePrice'] #, 'BaseVolume']\n",
    "learning_field_names = ['ClosePrice']\n",
    "\n",
    "target_market_names = None\n",
    "# target_market_names = ['NEOUSDT', 'LTCUSDT', 'BTCUSDT', 'ETHUSDT', 'BNBUSDT', 'QTUMUSDT', 'ADAUSDT', 'XRPUSDT']\n",
    "target_market_names = ['ETHUSDT']\n",
    "tarket_market_top_percent = 15\n",
    "\n",
    "Standardization = True\n",
    "Kill_Irregulars = True  # ----------------- pls implement it\n",
    "Time_into_X = True\n",
    "Time_into_Y = False #\n",
    "eFreeNoPlot = True\n",
    "\n",
    "#======================================================================== Model\n",
    "\n",
    "Num_Layers = 50 # Wow\n",
    "Num_Heads = 1   # As we have a single GPU, and we want to a exhaustic attention.\n",
    "Factor_FF = 4\n",
    "repComplexity = 12  # Wower\n",
    "Dropout_Rate = 0.  # You cannot change this once the model is built.\n",
    "\n",
    "dir_Checkpoint = os.path.join(dir_data, \"Checkpoints\")\n",
    "checkpoint_filepath = os.path.join(dir_Checkpoint, data_model)\n",
    "dir_CSVLogs = os.path.join(dir_data, \"CSVLogs\")\n",
    "csvLogger_filepath = os.path.join(dir_CSVLogs, data_model)\n",
    "\n",
    "#======================================================================== Train\n",
    "\n",
    "Epochs_Initial = 5000\n",
    "HuberThreshold = 4.0\n",
    "Checkpoint_Monitor = \"val_loss\"\n",
    "EarlyStopping_Min_Monitor = \"val_loss\"\n",
    "EarlyStopping_Patience = 5000\n",
    "\n",
    "Optimizer = \"adam\"\n",
    "LR_initial = 1.e-6 # default: 1e-4\n",
    "LR_rate = 0./100\n",
    "LR_skip = 0\n",
    "\n",
    "#=============================================================== Checksum\n",
    "\n",
    "params = data_model.split('.')\n",
    "assert int(params[1]) == int(CandleFile.split('-')[-1][:-1])\n",
    "assert int(params[2]) == Nx\n",
    "assert int(params[3]) == Ny\n",
    "assert int(params[4]) == min_true_candle_percent_x\n",
    "assert int(params[6]) == Num_Layers\n",
    "assert int(params[7]) == repComplexity\n",
    "targets = params[5]\n",
    "if targets.isnumeric():\n",
    "    assert target_market_names is None\n",
    "    assert int(targets) == tarket_market_top_percent\n",
    "else:\n",
    "    for target in targets.split(','):\n",
    "        assert (target+'usdt').upper() in target_market_names\n",
    "\n",
    "#============================================================== Consistency\n",
    "folders = [dir_data, dir_candles, dir_Checkpoint, dir_CSVLogs]\n",
    "for folder in folders:\n",
    "    if not os.path.isdir(folder):\n",
    "        os.mkdir(folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    print(gpus)\n",
    "    try:\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "            # tf.config.experimental.set_virtual_device_configuration(\n",
    "            #     gpu,[tf.config.experimental.VirtualDeviceConfiguration(memory_limit=5120)]) # why 5120?\n",
    "            # logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "            # print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)\n",
    "\n",
    "mirrored_strategy = None\n",
    "if len(gpus) > 1: \n",
    "    mirrored_strategy = tf.distribute.MirroredStrategy()\n",
    "    LR_initial = LR_initial * len(gpus) * 3 / 4\n",
    "\n",
    "# tf.config.experimental.set_virtual_device_configuration(\n",
    "#     gpus[0],[tf.config.experimental.VirtualDeviceConfiguration(memory_limit=5120)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==================== Load candle and reports. Data structure depends on TradingBot ====================\n",
    "Candles = np.load( os.path.join( dir_candles, \"table-\" + CandleFile + \".npy\") )\n",
    "with open( os.path.join( dir_candles, \"reports-\" + CandleFile + \".json\"), \"r\") as f:\n",
    "    reports = json.loads(f.read())\n",
    "\n",
    "#==================== Load candle and reports. Data structure depends on TradingBot ====================\n",
    "Candles2 = np.load( os.path.join( dir_candles, \"table-\" + CandleFile2 + \".npy\") )\n",
    "with open( os.path.join( dir_candles, \"reports-\" + CandleFile2 + \".json\"), \"r\") as f:\n",
    "    reports2 = json.loads(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "market = 5\n",
    "Show_Price_Volume_10(Candles[market, :, :], 1, 1, 500)\n",
    "Event_Free_Learning_Scheme_10(Candles[market, :, :], 3, 30, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==================== Format candles data ====================================\n",
    "Candles, CandleMarks, all_market_names, x_indices, y_indices, \\\n",
    "chosen_market_names_x, chosen_field_names_x, chosen_market_names_y, chosen_field_names_y, \\\n",
    "chosen_market_names, chosen_field_names, \\\n",
    "target_markets_names, target_markets = \\\n",
    "get_formed_data_3(\n",
    "        Candles, reports, All_Field_Names, \n",
    "        min_true_candle_percent_x, chosen_fields_names_x, min_true_candle_percent_y, chosen_fields_names_y,\n",
    "        target_market_names, tarket_market_top_percent\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Candles.shape)\n",
    "print(CandleMarks.shape)\n",
    "print(len(all_market_names))\n",
    "print(x_indices)\n",
    "print(y_indices)\n",
    "print(chosen_market_names_x)\n",
    "print(chosen_field_names_x)\n",
    "print(chosen_market_names_y)\n",
    "print(chosen_field_names_y)\n",
    "print(chosen_market_names)\n",
    "print(chosen_field_names)\n",
    "print(target_markets_names)\n",
    "print(target_markets)\n",
    "print(len(chosen_market_names_x), len(chosen_market_names_y), len(target_markets_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "market = all_market_names.index(\"ETHUSDT\")\n",
    "plt.plot(Candles[:, market, 0], label='eth', linewidth=1)\n",
    "market = all_market_names.index(\"BNBUSDT\")\n",
    "plt.plot(Candles[:, market, 0], label='bnb', linewidth=1)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Candles2, CandleMarks2, missing_names2 = get_conformed_data_3(\n",
    "    Candles2, reports2, chosen_market_names,\n",
    "    All_Field_Names, chosen_field_names\n",
    " )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "market = all_market_names.index(\"ETHUSDT\")\n",
    "joined = np.concatenate([Candles[:, market, 0], Candles2[:, market, 0]], axis=0)\n",
    "plt.plot(joined, label='eth', linewidth=1)\n",
    "market = all_market_names.index(\"BNBUSDT\")\n",
    "joined = np.concatenate([Candles[:, market, 0], Candles2[:, market, 0]], axis=0)\n",
    "plt.plot(joined, label='bnb', linewidth=1)\n",
    "if len(missing_names2) > 0:\n",
    "    market = all_market_names.index(missing_names2[0])\n",
    "    joined = np.concatenate([Candles[:, market, 0], Candles2[:, market, 0]], axis=0)\n",
    "    plt.plot(joined, label=\"1st missing: \" + missing_names2[0][:-len('USDT')].lower(), linewidth=1)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=========================== Get time features ======================\n",
    "start_ts, interval_s, timestamps_abs, Times = get_joint_time_features_2(CandleFile, Candles.shape[0], Candles.dtype)\n",
    "\n",
    "# start_ts2, _, timestamps_abs2, Times2 = get_joint_time_features_2(CandleFile2, Candles2.shape[0], Candles2.dtype)\n",
    "# assert timestamps_abs2[0] - timestamps_abs[-1] == interval_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(Candles.shape, Times.shape)\n",
    "# print(timestamps_abs[:5])\n",
    "# print(Candles2.shape, Times2.shape)\n",
    "# print(timestamps_abs2[:5])\n",
    "# # print(Times[:5])    # Agnostic of restart of this training notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#========================= Get sample anchors ==============================\n",
    "sample_anchors_t, sample_anchors_v = get_sample_anchors_3(Candles, Nx, Ny, Ns, seed=523)\n",
    "print(sample_anchors_t.shape, sample_anchors_v.shape)\n",
    "print(sample_anchors_t[:5])\n",
    "assert sample_anchors_t[0] == 366760 \n",
    "assert sample_anchors_t[1] == 118030\n",
    "\n",
    "sample_anchors_t2, sample_anchors_v2 = get_sample_anchors_3(Candles2, Nx, Ny, Ns, seed=523)\n",
    "print(sample_anchors_t2.shape, sample_anchors_v2.shape)\n",
    "print(sample_anchors_t2[:5])\n",
    "assert sample_anchors_t2[0] == 5365 \n",
    "assert sample_anchors_t2[1] == 8585"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "JointCandles, JointMarks, sample_anchors_t, sample_anchors_v = \\\n",
    "    get_joint_data(\n",
    "        Candles, CandleMarks, sample_anchors_t, sample_anchors_v,\n",
    "        Candles2, CandleMarks2, sample_anchors_t2, sample_anchors_v2\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(JointCandles.shape)\n",
    "print(sample_anchors_t.shape, sample_anchors_v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_ts, interval_s, timestamps_abs, JointTimes = get_joint_time_features_2(CandleFile, JointCandles.shape[0], JointCandles.dtype)\n",
    "size_time = Times.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#======================== Get event-free data ========================\n",
    "\n",
    "org_length = JointCandles.shape[0]\n",
    "JointCandles, JointMarks, JointTimes = get_eventfree_data(\n",
    "    JointCandles, JointMarks, JointTimes,\n",
    "    SmallSigma, LargeSigma, all_market_names, chosen_market_names, \n",
    "    All_Field_Names, chosen_field_names,\n",
    "    eFreeNoLog, eFreeNoPlot, Volume_Volatility\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert JointCandles.shape[0] == JointMarks.shape[0]\n",
    "assert JointCandles.shape[0] == JointTimes.shape[0]\n",
    "print(JointCandles.shape, CandleMarks.shape, JointTimes.shape, org_length-JointCandles.shape[0])\n",
    "# print(Candles[-3:]) # Agnostic of restart of this training notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#========================= Standardize data ============================\n",
    "Standard = None\n",
    "if Standardization:\n",
    "    JointCandles, Standard = standardize_2(JointCandles)\n",
    "# print(Candles[-3:]) # Agnostic of restart of this training notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(16,3))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.set_title(\"Features are custom-standardized\" if Standardization else \"Features are not standardized\")\n",
    "for market in range(JointCandles.shape[1]):\n",
    "    for field in range(JointCandles.shape[2]):\n",
    "        ax.plot(JointCandles[:, market, field], label = \"{} @ {}\".format(All_Field_Names[field], all_market_names[market][:-len('USDT')]), lw=0.1)\n",
    "ax.legend(loc = 'upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_mask = [1.0]\n",
    "\n",
    "ds_train, ds_valid, dx, dy = \\\n",
    "get_datasets_no_end_single_point(\n",
    "    JointCandles, Time_into_X, Time_into_Y, JointTimes, \n",
    "    sample_anchors_t, sample_anchors_v,\n",
    "    Nx, x_indices, Ny, y_indices, size_time, target_markets, learning_mask, Shift,\n",
    "    BatchSize, shuffle_batch, shuffle=(len(gpus)<=1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = None\n",
    "\n",
    "if mirrored_strategy is None:\n",
    "    model = build_model_3(\n",
    "        dx, dy, Num_Layers, Num_Heads, Factor_FF, repComplexity, Dropout_Rate,\n",
    "        HuberThreshold, Optimizer, initial=LR_initial, rate=LR_rate, skip=LR_skip\n",
    "    )\n",
    "else:\n",
    "    with mirrored_strategy.scope():\n",
    "        model = build_model_3(\n",
    "            dx, dy, Num_Layers, Num_Heads, Factor_FF, repComplexity, Dropout_Rate,\n",
    "            HuberThreshold, Optimizer, initial=LR_initial, rate=LR_rate, skip=LR_skip\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = get_callbacks(\n",
    "    checkpoint_filepath, Checkpoint_Monitor, \n",
    "    csvLogger_filepath, \n",
    "    EarlyStopping_Min_Monitor, EarlyStopping_Patience\n",
    ")\n",
    "try:\n",
    "    model.load_weights(checkpoint_filepath)\n",
    "    print(\"Loading a checkpoint...done\")\n",
    "except:\n",
    "    print(\"No checkpoint to load. Fitting initailly...\")\n",
    "    model.fit(\n",
    "        ds_train, # x and y_true\n",
    "        validation_data=ds_valid,\n",
    "        epochs=1, #Epochs_Initial,\n",
    "        callbacks=callbacks\n",
    "    )\n",
    "try:\n",
    "    columns = ('loss', 'val_loss', 'mTA', 'val_mTA')\n",
    "    plot_csv_train_history(csvLogger_filepath, columns, title=data_model)\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(\n",
    "    ds_train, # x and y_true\n",
    "    validation_data=ds_valid,\n",
    "    epochs=Epochs_Initial,\n",
    "    callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ('loss', 'val_loss', 'mTA', 'val_mTA')\n",
    "plot_csv_train_history(csvLogger_filepath, columns, title=data_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==================== Load candle and reports. Data structure depends on TradingBot ====================\n",
    "Candles3 = np.load( os.path.join( dir_candles, \"table-\" + CandleFile3 + \".npy\") )\n",
    "with open( os.path.join( dir_candles, \"reports-\" + CandleFile3 + \".json\"), \"r\") as f:\n",
    "    reports3 = json.loads(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Candles3, CandleMarks3, missing_names3 = get_conformed_data_3(\n",
    "    Candles3, reports3, chosen_market_names,\n",
    "    All_Field_Names, chosen_field_names\n",
    " )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Candles3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_ts3, interval_s, timestamps_abs3, Times3 = \\\n",
    "    get_standalone_time_features(CandleFile, CandleFile3, Candles3.shape[0], Candles3.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#======================== Get event-free data ========================\n",
    "\n",
    "org_length = Candles3.shape[0]\n",
    "Candles3, CandleMarks3, Times3 = get_eventfree_data(\n",
    "    Candles3, CandleMarks3, Times3,\n",
    "    SmallSigma, LargeSigma, all_market_names, chosen_market_names, \n",
    "    All_Field_Names, chosen_field_names,\n",
    "    eFreeNoLog, eFreeNoPlot, Volume_Volatility\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if Standardization:\n",
    "    Candles3 = standardize_observation(Candles3, Standard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del Candles, Candles2\n",
    "\n",
    "step = Ny\n",
    "pred_base_ts = timestamps_abs3[ np.array(range(timestamps_abs3.shape[0]-1, 0, -step) , dtype=np.int64) ]\n",
    "Candles3_padded = np.pad(Candles3, [[0, Ny], [0,0], [0,0]], constant_values=0)\n",
    "print(Candles3.shape, Candles3_padded.shape)\n",
    "Times3_padded = np.pad(Times3, [[0, Ny], [0,0]], constant_values=0) # pad with 0 assuming Time_into_Y == False\n",
    "_sample_anchors_t = np.flip(np.array(range(Candles3_padded.shape[0] - Nx - Ny, 0, -step), dtype=np.int64), axis=0)\n",
    "_sample_anchors_v = np.array([], dtype=np.int64)\n",
    "_shift = 0; _batchSize = BatchSize ; _shuffle_batch = 0; _shuffle = False\n",
    "_ds_train, _, _, _ = \\\n",
    "get_datasets_3(\n",
    "    Candles3_padded, Time_into_X, Time_into_Y, Times3_padded,\n",
    "    _sample_anchors_t, _sample_anchors_v,\n",
    "    Nx, x_indices, Ny, y_indices, size_time, target_markets, _shift,\n",
    "    _batchSize, _shuffle_batch, shuffle=_shuffle, cache=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for anchor_id in range(_sample_anchors_t.shape[0]):\n",
    "#     print(anchor_id)\n",
    "#     size_y = get_timepoint_size(y_indices)\n",
    "#     anchor = _sample_anchors_t[anchor_id]\n",
    "#     y_anchor = Candles3_padded[anchor, y_indices[0], y_indices[1] ]\n",
    "#     time_anchor = Times3_padded[anchor]\n",
    "\n",
    "#     b_cnt = anchor_id // _batchSize\n",
    "#     for ((x, y), _) in _ds_train:\n",
    "#         if b_cnt == 0:\n",
    "#             x = x.numpy(); y = y.numpy()\n",
    "#             assert (x[anchor_id % _batchSize, 1, : size_y] == y_anchor).all()  # (batch, Nx, dx)\n",
    "#             assert (x[anchor_id % _batchSize, 1, size_y : -1] == time_anchor).all()  # (batch, Nx, dx) time_features\n",
    "#             break\n",
    "#         else:\n",
    "#             b_cnt -= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anchor_start = _sample_anchors_t[0]\n",
    "\n",
    "Candles3_pred = Candles3_padded.copy() # np.zeros_like(Candles3_padded, dtype=Candles3_padded.dtype)\n",
    "Candles3_pred = np.zeros_like(Candles3_padded, dtype=Candles3_padded.dtype)\n",
    "\n",
    "anchor_id = 0\n",
    "size_y = get_timepoint_size(y_indices)\n",
    "for ((x, y), _) in _ds_train: # must loop Ny + 1\n",
    "    print(anchor_id, end=' ')\n",
    "    x = x.numpy(); y = y.numpy(); y[:] = 0.0  # (batch, Ny+1, dy)  y[:, 0, :] == Start\n",
    "    # print(x.shape, y.shape)\n",
    "    for i in range(y.shape[1]):\n",
    "        pred = model((x, y))    # pred: (batch, Ny+1, dy)\n",
    "        pred = pred.numpy()\n",
    "        y[:, i] = pred[:, i]\n",
    "\n",
    "    for b in range(x.shape[0]):\n",
    "        anchor = _sample_anchors_t[anchor_id]\n",
    "        print(anchor, end=' ')\n",
    "        Candles3_pred[anchor + Nx: anchor + Nx + Ny] = y[b, :-1, :size_y].reshape((Ny, len(y_indices[0]), len(y_indices[1])))\n",
    "        # print(b, anchor_id, anchor)\n",
    "        anchor_id += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "market_name = \"ETHUSDT\"\n",
    "market_id = all_market_names.index(market_name)\n",
    "field_id = 0\n",
    "print(market_id)\n",
    "\n",
    "points = tuple(range(Candles3_padded.shape[0]))\n",
    "lw = 0.5\n",
    "plt.plot(Candles3_padded[:, market_id, field_id ], label='true', lw=lw)\n",
    "plt.plot(Candles3_pred[:, market_id, field_id ], label='pred', lw=lw)\n",
    "plt.legend()\n",
    "plt.title(market_name)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(timestamps_abs3[-1])\n",
    "print(datetime.fromtimestamp(timestamps_abs3[-1]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
